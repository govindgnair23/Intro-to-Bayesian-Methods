---
title: "Introduction to Gaussian Processes"
author: "Govind G Nair"
date: "7/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Gaussian Distributions

Fundamental to understanding Gaussian process in a Gaussian distribution, the 1 dimensional version of which is the Normal distribution or the Bell Curve.

The plot of a standard normal distribution with mean 0 and variance 1 is shown below. Note that the curve doesn't look like a perfect bell as the data has been simulated
```{r cars}
set.seed(0)
plot(density(rnorm(100000)),main = bquote("N("~mu~"=0,"~sigma^2~"=1)"))
```


A bivariate gaussian distribution is simply a Guassian distribution in 2 dimensions. Whereas a univariate Gaussian distribution is parameterized by two scalar quantites (i.e. the mean and variance),the bivariate Gaussian distribution and its higher dimensional counterparts are parameterised by a vector( the mean) and a matrix (the covariance matrix)

$$\begin{bmatrix} x_1\\x_2 \end{bmatrix} = N\Big( \begin{bmatrix} \mu_1\\\mu_2 \end{bmatrix},\begin{bmatrix} \Sigma_{11} & \Sigma_{12}\\\Sigma_{21} & \Sigma_{22} \end{bmatrix} \Big)$$

A bivariate gaussian can be represented using a simple scatter plot as shown below.

```{r}
library(ggplot2)
df = data.frame(x= rnorm(10000),y =rnorm(10000))
ggplot(df,aes(x=x,y=y)) + geom_point()+geom_density_2d()+labs(x='x1',y='x2')
```

Note that the bivaraiate gaussian above is constructed using two independant 1D gaussians, meaning the two variables being plotted here are independant i.e. information about one doesn't tell us anything about the other. This is further evidenced by the circular, symmetrical distribution of the points.

Such a standard normal bi-variate gaussian has mean $\mu = \begin{bmatrix} 0\\0 \end{bmatrix}$ and a covariance matrix $\begin{bmatrix} 1 & 0\\0 & 1 \end{bmatrix}$

Note the off-diagonal elements of the covariance matrix are 0.

The plot below shows a bivariate gaussian with $\mu = \begin{bmatrix} 0\\0 \end{bmatrix}$ and a covariance matrix $\begin{bmatrix} 1 & 0.7\\0.7 & 1 \end{bmatrix}$. 

```{r}

gaussian_2d <- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,0.7,0.7,1),ncol=2)))
ggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() 

```

Here the two random variables are positively correlated and  are not independant,because if one is positive, we know that the second is also far more likely to be positive rather than negative

Similarly,the plot below shows a bivariate gaussian with $\mu = \begin{bmatrix} 0\\0 \end{bmatrix}$ and a covariance matrix $\begin{bmatrix} 1 & -0.7\\-0.7 & 1 \end{bmatrix}$

```{r , warning=FALSE, message=FALSE}
library(mvtnorm)
gaussian_2d <- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,-0.7,-0.7,1),ncol=2)))
ggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() 
```

Here the two variables are negatively correlated.

Now consider a different visualization of the same data in the figure above.Only 10 of the 10,000
points plotted above is visualized here.

```{r}
library(gganimate)
library(tidyr)
library(dplyr)

#Select top 10 rows for visualization
viz_data <- head(gaussian_2d,10)
viz_data$set <- as.integer(c(1:10))
#Convert to long format
viz_data_long <- gather(viz_data,key='datapoint',value='y',-set) %>% arrange(set) %>% 
                    select(-datapoint) %>% 
                      mutate(x = rep(c(1,2),length(unique(viz_data_long$set)))) %>%
                        select(set,x,y)


ggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + geom_line(col='cornflowerblue')+
  transition_time(set)


```

The 2D gaussian distribution can be used to model a line between two given points. A gaussian distribution
qith N dimesnions can be used to model a line through N different points as shown below.

A 10 D gaussian with a covariance matrix shown below can be constructed.


```{r}
N  <- 10 # dimension of required cov matrix
cov_mat <- diag(N)

vec <- seq(1,0.1,by = -1/N)[2:N]

for(i in 1:(N-1)){
  cov_mat[i,(i+1):N] <- cov_mat[(i+1):N,i] <- vec[1:(N-i)]
}

rownames(cov_mat) <- colnames(cov_mat)<- paste0('X',c(1:10))
print(cov_mat)
```

```{r}
gaussian_10d <- data.frame(rmvnorm(10,mean= rep(0,10),sigma = cov_mat))
viz_data <- gaussian_10d
viz_data$set <- as.integer(c(1:10))
#Convert to long format
viz_data_long <- gather(viz_data,key='datapoint',value='y',-set) %>% arrange(set) %>% 
                    select(-datapoint) %>% 
                      mutate(x = rep(c(1:10),length(unique(viz_data_long$set)))) %>%
                        select(set,x,y)


ggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + geom_line(col='cornflowerblue')+
  transition_time(set)

```


This should give you the intuition that using an N - dimensional gaussian can be used to model a line through N number of points and an infinite dimensional gaussian can model a continious curve. This leads to the idea for gaussian process regression.

Also compare the covariance between any two points  in the covaraince matrix above and how it manifests in the graph. X1 and X2 have a high covariance, hence the points at 1 and 2 on the x-axis are close to each other along the y-axis. X1 and X10 on the other hand have a low covariance,hence the points 1 and 10 on the x-axis can vary widely. 

Conversely if two points lie close to each other on the y-axis, they are likely to have a high covariance whereas if 2 points lie far apart from each other on the y-axis, they are likely to have a low covariance.



##Marginal Distributions and Conditional Distributions


Given a jointly Gaussian distribution

$$\begin{bmatrix} x_1\\x_2 \end{bmatrix} = N\Big( \begin{bmatrix} \mu_1\\\mu_2 \end{bmatrix},\begin{bmatrix} \Sigma_{11} & \Sigma_{12}\\\Sigma_{21} & \Sigma_{22} \end{bmatrix} \Big)$$

according to the **Multivariate Gaussian Theorem**, the marginal distributions of the two component variables are given by.

$$ p(x_1) = N (x_1 | \mu_1.\Sigma_{12}) $$

$$ p(x_2) = N (x_2 | \mu_2.\Sigma_{22}) $$

The conditional distribution is given by:

$$ p(x_2|x_1) = N (x_2 | \mu_{2|1},\Sigma_{2|1}) $$
where:

$$ \mu_{2|1} = \mu_2 + \Sigma_{21}\Sigma_{11}^{-1} (x_1 - \mu_1) $$

and

$$ \Sigma_{2|1} = \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12} $$

These expression are critical to do inference with Gaussian Process Regression.


## Modelling data using Gaussian Distributions.

Consider three points shown below which you want to model using a Gaussian distribution.


```{r}

plot(data.frame(x = c(1,1.5,3) , f = c(0.2,0.25,0.8)),xlim=c(0,5),ylim = c(0,1),pch=16)
text(x=c(1,1.5,3),y = c(0.25,0.3,0.85),labels=c('(x1,f1)','(x2,f2)','(x3,f3)'))
```

The three points above can be modelled using a Gaussian distribution of the form below. Note that a zero vector can be used for the mean after standardazing the data.

$$\begin{bmatrix} f_1\\f_2\\f_3 \end{bmatrix} = N\Bigg( \begin{bmatrix} 0\\0\\0 \end{bmatrix},\begin{bmatrix} K_{11} & K_{12} & K_{13}\\K_{21} & K_{22} & K_{23}\\K_{31} & K_{32} & K_{33} \end{bmatrix} \Bigg) = N(\textbf{0},\textbf{K})$$



To model these data points using a Gausssian distribution , we need to construct a covaraince matrix such that the terms $K_{ij}$ captures the similarity or coavraince between points i and j.

The similarity betwewn two points can be captured using kernel functions.Consider a squared exponential kernel that captures the similarity between two points $X_i$ and $X_j$

$$K_{ij} =  e^{ -\sigma || X_i - X_j ||^2}$$ 



$$
K_{ij} = \begin{cases} 
                     0, \text{if}\ ||X_i -X_j|| \to \infty \\
                     1, \text{if}\ X_i = X_j
          \end{cases}
          
$$


The covariance matrix for the three points can be calculated as shown below

```{r}
library(kernlab)
rbfkernel <- rbfdot(sigma = 0.1) # Set value of hyperparameter
point1 <- c(1)
point2 <- c(1.5)
point3 <- c(3)

K12 <- K21 <- rbfkernel(point1,point2)
K23 <- K32 <- rbfkernel(point2,point3)
K13 <- K31 <-rbfkernel(point1,point3)
K11 <- rbfkernel(point1,point1)
K22 <- rbfkernel(point2,point2)
K33 <- rbfkernel(point3,point3)


K <- matrix(c(K11,K12,K13,K21,K22,K23,K31,K32,K33),byrow=TRUE,nr=3)
print(K)

```


Now that we have learned a model to fit the three points, we want to predict the value of a fourth point $x^*$, the predicted value for this point $f^*$ can lie anywhere along the red vertical lines.

```{r}
plot(data.frame(x = c(1,1.5,3) , f = c(0.2,0.25,0.8)),xlim=c(0,5),ylim = c(0,1),pch=16)
abline(v= 2.5,col='red')
text(x=c(1,1.5,3,2.15),y = c(0.25,0.3,0.85,0.5),labels=c('(x1,f1)','(x2,f2)','(x3,f3)','(x*=2.5,f*)'))
```

In order to capture the relationship between the four points, the four points  can be modelled as a 4D Gaussian defined as follows:

$$\begin{bmatrix} \textbf{f}\\f^* \end{bmatrix} = N\Bigg( \begin{bmatrix} \textbf{0}\\0 \end{bmatrix},\begin{bmatrix} \textbf{K} & \textbf{K}_*\\\textbf{K}_*^T & K_{**} \end{bmatrix} \Bigg) $$

where:

$ \textbf{f} = \begin{bmatrix} f_1\\f_2\\f_3 \end{bmatrix}$,$ \textbf{0} = \begin{bmatrix} 0\\0\\0 \end{bmatrix}$, $ \textbf{K} =\begin{bmatrix} K_{11} & K_{12} & K_{13}\\K_{21} & K_{22} & K_{23}\\K_{31} & K_{32} & K_{33} \end{bmatrix}$ and $\textbf{K}_* = \begin{bmatrix} K_{1*}\\K_{2*}\\K_{3*} \end{bmatrix}$


This is a joint distribution over $\textbf{f}$ and $f^*$. Given $\textbf{f}$ which we already know from the three available datapoints, we can use the **Multivariate Gaussian Theorem** to estimate the distribution over $f_*$. The mean and variance of this distribution is goven by:

$$ \mu^* = \textbf{K}_*^{T}\textbf{K}^{-1}f$$

 $$ \sigma^* = K_{**} - \textbf{K}_*^{T} \textbf{K}^{-1} \textbf{K}_* $$
 
We can get  mean and variance of the distribution over the unknown value at x = 2.5 as follows

```{r}

f <- c(0.2,0.25,0.8)

point_star <- c(2.5)

K1star <- Kstar1 <- rbfkernel(point1,point_star)
K2star <- Kstar2 <- rbfkernel(point2,point_star)
K3star <- Kstar3 <-rbfkernel(point3,point_star)
Kstarstar <- rbfkernel(point_star,point_star)

Kstar = c(K1star,K2star,K3star)
Kstar_t = t(Kstar)
K_inv = solve(K)

##The mean##
mu_star = Kstar_t%*%K_inv%*%f

##The  standard deviation##
sigma_star = sqrt(Kstarstar - Kstar_t%*%K_inv%*%Kstar)

print(paste0('Mean of f* = ',mu_star))
print(paste0('Standard deviation of f* = ',sigma_star))

```

The prediction is shown below in green using an error bar.

```{r}
plot(data.frame(x = c(1,1.5,3,point_star) , f = c(0.2,0.25,0.8,mu_star)),xlim=c(0,5),ylim = c(0,1),pch=16, col = c(rep('black',3),'green'))
arrows(point_star,mu_star - 1.96*sigma_star,point_star,mu_star + 1.96*sigma_star,length = 0.05,angle=90,
       code=3,col='green')


```


Similarly the prediction of any point along the x-axis can be found. Below the predictions for a sequence of points along the x-axis have been generated and plotted along with the their 95% confidence intervals.


```{r}

predict <- function(input){
  
  point_star <- input

  K1star <- Kstar1 <- rbfkernel(point1,point_star)
  K2star <- Kstar2 <- rbfkernel(point2,point_star)
  K3star <- Kstar3 <-rbfkernel(point3,point_star)
  Kstarstar <- rbfkernel(point_star,point_star)

  Kstar = c(K1star,K2star,K3star)
  Kstar_t = t(Kstar)
  K_inv = solve(K)

  ##The mean##
  mu_star = Kstar_t%*%K_inv%*%f

  ##The standard deviation##
  sigma_star = tryCatch(sqrt(Kstarstar - Kstar_t%*%K_inv%*%Kstar),
                        warning = function(e){0})
  
  return(c(mu_star,sigma_star))
}

##Vector of points to generate predictions on
x <- seq(0,5,by=0.1)

predictions <- sapply(x,predict)
mean <- predictions[1,]
#lower bound
lb <- predictions[1,] - 1.96*predictions[2,]
#upper bound
ub <-predictions[1,] + 1.96*predictions[2,]
#Bind together in a dataframe
preds <- data.frame(cbind(x,mean,lb,ub))
#Inputs
inputs <-  data.frame(x=c(point1,point2,point3),f=f)


ggplot(preds,aes(x=x,y=mean)) + 
  geom_ribbon(aes(x=x,ymin=lb,ymax=ub),fill='grey70')+
  geom_line(col='green')+
  geom_point(data=inputs,mapping=aes(x=x,y=f))+labs(y='y')
  

```


We can clearly see that the uncertainity near the training data points is low, as we move away from the training data points, the level of uncertainity increases. This aligns very well with our intuition.


## Gaussian Processes

The Guassian process is thus the generalization of a multi-variate normal distribution to infinitely many variables. It is a collection of random variables, any finite number of which will have consistent Gaussian distributions.

A Gaussian process is fully specified by a mean function $m(x)$ and a covariance function $K(x,x')$, it is a gaussian distribution over functions.













